\documentclass{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fancybox}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{listings}
\geometry{a4paper, margin=1in}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\setlength\parindent{0pt}
\usepackage{catchfilebetweentags}
\usepackage{graphicx}
\usepackage{float} % Helps keep images where you want them
% --- 1. Define the Listing Style (Optional, but highly recommended) ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolor}{rgb}{0.95,0.95,0.95}
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolor},
	commentstyle=\color{codegreen},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breaklines=true,
	captionpos=b, % Caption position at bottom
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}
\lstset{style=mystyle}
\graphicspath{ {fig/} {./images/} }
% 2. Your Custom Command
% Arguments:
% #1 = Relative path to file (e.g., ../code/tokenizer.py)
% #2 = The Tag Name (e.g., bpetraining)
% #3 = Caption
% #4 = Label (for referencing)
\newcommand{\gitcode}[4]{%
	\lstinputlisting[%
	language=Python,
	rangeprefix=\#\ <,%       Define the start of the comment (# <)
	rangesuffix=>,%           Define the end of the comment (>)
	includerangemarker=false, % Hide the comments in the PDF
	linerange=#2-#2,%         Find the block between the two identical tags
	caption={#3},%
	label={#4}%
	]{../code/#1}%
}

% Command: \addfig
% Arguments:
% #1 = Filename (e.g., plot.png)
% #2 = Caption
% #3 = Label (for referencing)
% Now the command is simpler and cleaner
\newcommand{\simplefig}[3]{%
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.9\linewidth]{#1} % No path needed here!
		\caption{#2}
		\label{#3}
	\end{figure}
}
\begin{document}

	\justifying
	
	\centering
	\title{Training models in Deep Learning: A Formal basis}
	\author{Samir El Karrat Moreno}
	\maketitle
	\justifying

	\tableofcontents % This command generates the table of contents/index
	
	\newpage % Start the content on a new page
	
	\section{Introduction} % Placeholder for an initial section if needed
	
	% --- Project Sections from your outline ---
	
	\section{Input data}
	
	\newpage
	\section{Tokenization}
	
	
	% --- End of Project Sections ---
	
	In DL we want to train our models by finding the minima of a chosen loss function. Let us denote $x^{(i)}\in \mathcal{D}_{\text{input}}$ be our input data $i = 1, \dotsc, n$ in the input space $\mathcal{D}_{\text{input}}$ for, $n$ being the amount of data that we are training our model with. Often times our input data $x^{(i)}$ can be highly dimensional, possibly reducing the context window of our models (see \autoref{subsec:Transformers}). For example, CNNs work directly on the pixel tensor representing an image. However, for when having text as our input data several methods of compression have been developed, as we are going to describe in the following discussion.
	
	In the case that we want to represent our data more condensedly, it is popular to take the following steps: 
	
	\begin{itemize}
		\item[(1)] \textbf{Tokenization}: It is a 'statistical' model for encoding input data following certain fixed rules and frequency optimizations to create condensed representations called \textbf{token sequences}. The latter consist of strings of \textbf{tokens}, statistically optimized encoded subdivisions of our original data. This process is bi-directional, and given any token sequence we can recover its input space representation.
		
		During the training of a tokenizer, we look for efficient mappings between tokens in order to compress token sequences further. We denote the concept of \textbf{vocabulary} as the collection of all such mappings. It makes up for the entirety of the learnt parameters. However the \textbf{vocabulary dimension} is a hyper-parameter.
	\end{itemize}
	
	\simplefig{tokenizerc.png}{How tokenizer works}{fig:tokenizerfig}
	
	\subsection{Byte Pair Encoding (BPE)}
	
	\begin{comment}
		Byte Pair Encoding, or shortly BPE, is a basic text tokenization method that first uses an UTF-8 encoder (most frequently) to translate strings into raw bytes. Then, the most frequent pairs of subsequent bytes get added to a vocabulary. Each new byte-pair added is called a \textbf{merge}. We perform this operation iteratively, further indexing our token sequence according to new merges. In this way, merges defined as combinations of new indices plus some regular bytes are possible.
		
		The process described before is used to \emph{train} the model. We can use any form of text as input data. However, as we tipically want a versatile tokenizer for LLMs, it is beneficial that the training data correctly represents expected inputs during evaluation. For this reason we must make sure that training input texts contain a combination of code and different languages, making a 'uniform vocabulary density' without over-specializing in any domain.
		
		Let us now look at a python implementation of this algorithm:
	\end{comment}
	
	Byte Pair Encoding, or shortly BPE, is a basic example of text tokenization method. During its training stage, we first find the most frequent bigram (tuple of subsequent tokens) in the corresponding token sequence of our training text. Then we add it to our vocabulary, creating in this way a new \textbf{merge}. After this we replace all its occurrences with the correspnding mapping according to the vocabulary, i.e, its index. We perform the whole process iteratively, adding new merges until the desired vocabulary size is achieved. Note that merges containing tokens previously added to the vocabulary are a possiblity. A possible implementation could be as described in \autoref{alg:bpe}:
	\begin{algorithm}
		\caption{Byte Pair Encoding using UTF-8 (BPE) -- Training}
		\label{alg:bpe}
		\begin{algorithmic}[1]
			\Require Input text sequence $T$, vocabulary size $V_{target}$
			\Ensure Learned merge rules $M$, Final token sequence $S$
			
			\State Convert $T$ into a sequence of UTF-8 bytes: $S \gets [b_1, b_2, \dots, b_n]$ where $b_i \in [0, 255]$
			\State Initialize vocabulary size $V \gets 256$
			\State Initialize merge rules $M \gets \emptyset$
			
			\While{$V < V_{target}$}
			\State $C \gets \text{CountPairs}(S)$ \Comment{Compute frequency of adjacent pairs}
			\If{$C$ is empty}
			\State \textbf{break}
			\EndIf
			\State $(p_1, p_2) \gets \arg\max_{(x, y) \in C} C[x, y]$ \Comment{Find most frequent pair}
			\State $idx \gets V$
			\State $M[(p_1, p_2)] \gets idx$ \Comment{Record the new merge rule}
			\State $S \gets \text{Merge}(S, (p_1, p_2), idx)$ \Comment{Replace all occurrences in sequence}
			\State $V \gets V + 1$
			\EndWhile
			\State \textbf{return} $M, S$
		\end{algorithmic}
	\end{algorithm}
	\newpage
	During BPE's inference stage, we encode/decode any token sequence from an inference text (the text we want to tokenize). For encoding, we simply just loop through each merge in our vocabulary, hierarchically from first merges to last merges, replacing any ocurrence of it with its assigned token. For decoding, we perform the same loop but in the opposite order, from last performed merges to the first ones, replacing them for their corresponding bigram according to the vocabulary. See a pseudocode implementation in \autoref{alg:bpe-extra}.
		
	Usually, we do not work with a raw string of text. We instead format it in a general way that allows us to uniformly treat basically any string of text we can ever encounter. This can be done in various forms, like for example formatting to bytes using an UTF-8 encoder. The reference library for it is OpenAI's official library for inference-only tokenization, \textbf{tiktoken}. We see a small snippet in \autoref{lst:tiktoken}:
	
	\gitcode{tokenization_libraries.py}{tiktoken}{Example usage of the tiktoken library, for GPT-2 and GPT-4 tokenizers.}{lst:tiktoken}
	
	However, this is not the most common library. Instead it is \textbf{sentencepiece}, a widely used BPE tokenizer that works directly on the text's Unicode \textit{code points}, i.e., the characters of the text seen as a string. Note that exclusively building the vocabulary using code points (CPs from now on) and its mergings lacks generalization. CPs not seen during training would lack an entry in our vocabulary, a problem we would encounter using UTF-8 pre-encoding. Sentencepiece overcomes the challenge by providing a \textit{character coverage}. It is a hyper-parameter that determines what percentage of the training CPs won't be treated as unseen. There are two approaches for dealing with unseen CPs:
	
	\begin{itemize}
		\item[(1)] 'UNK' Mapping: Mapping them to the special token 'UNK'.
		\item[(2)] Byte Fallback: Pre-encoding them using UTF-8, previously introducing bits in the vocabulary.
	\end{itemize}
	
	\gitcode{tokenization_libraries.py}{sentencepiece}{Example usage of the sentencepiece library, reproducing LLama2 tokenizer }{lst:spm}
	
	As we can see in \autoref{lst:spm}, we will be working with text files. We define different hyper-parameters: vocabulary size \& character coverage with byte fallback. Moreover we set different merging rules for segmentation of the text and add special tokens.
	
	

	\subsubsection{Regex patterns \& special tokens}
	
	
	 After this talk about regex motivated by token issues. Continue with special tokens (useful for meta data, used in finetuning like adapting to browser)


	\section{Embeddings}
	https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/
	
	\subsection{Text embeddings}
	
	In what follows we will overview NLP Theory.
	
	We want to encapsulate de meaning of a word $w$ by encoding it as an \textbf{embedding} or word representation $w\in \mathbb{R}^n$, for some relatively low dimensional $n$. Ideally, we want words that are close to each other in meaning, i.e., \textbf{semantically similar} words, to have corresponding vectors that are "close in distance".
	
	First attempts of generating embeddings consist comprise the so called \textit{one hot encoding} technique. Its approach is to assign words to each of the different components of the vectors in space. We are using \textbf{localist representations} for words, as the $j$-th component of any given vector represents an specific word, so we can say the vector $e_j$ is that exact word. Following this reasoning, the canonical base of our space is formed by all the embeddings of our words in the vocabulary. Specifically, this means all the embeddings are orthogonal, meaning there is no point in encoding meaning as their dot product or distance. In general, any efforts of encoding meaning in this framework are not of interest inefficient to perform.
	
	Current trends involve \textbf{distributed representations} for words. Embeddings take the form of \textbf{dense vectors}, which allows use to measure their semantic similarity via their dot product. Unlike one hot encoding, we no longer encode words to components. We pay attention to the first efforts on creaing such representations using the now dated framework called \textit{word2vec}. It will be a good intuition for understanding advanced embedding frameworks like BERT and ELMo.

	
	\subsubsection{Word2vec}
	
	Word2vec is a framework comprising models that create embeddings as distributed representations. The main assumption of this model is that words that appear often in similar contexts, i.e., often surrounded by a similar set of words around them, then they must be semantically similar. Despite slight differences, algorithmically the models share the same following structure:
	
	We start with a \textbf{corpus} which is our main body text and fix a vocabulary with all of the words that appear in it (wihout repetition). Then, we iterate through each word with position $t$ in our corpus $w_t$, considering a word context window of size $s$. Together with $w_t$, we conider its set of surrounding words $\{w_{t+j},  j =\pm 1, \pm 2, ..., \pm s \}$. Depending on the model, 
	
	
	The main two models are the \textit{Continuous Bag Of Words} and \textit{Skip-Gram}. They
	
	
	 They propose different objective functions.
	
	
	
	Desribe idea behind word2vec Math: correspondance to MLE and the approach to model the probabilities
	
	
	Information Theory: High Frequency = Low Information (ALSO IN THIS WEEK'S PAPER!)
		
	\subsection{Image embeddings (put after NN )}
	Focus on ViT and maybe compare to some CNN approach
	
	
	training by distillation and finetuning
	
	\section{Datasets \& Data Loading in Pytorch}
	
	\section{Deep neural networks: mathematical formulation}
	\subsection{Bias \& Affine transformation}
	\subsection{Activation functions}
	\subsection{Logits \& Softmax}
%	
	\section{Types of neural networks}
	
	\section{Training}
	\subsection{Mathematical motivation (via Statistics)}
	
	
	\subsubsection{Maximum Likelihood Estimation}
	The \textbf{Maximum Likelihood Estimation} is an \emph{statistical approach} to the following problem: given some IID (independent and identically distributed) from which we know what distribution they follow but not its parameters, how can we \emph{estimate} them.
	
	MLE solves this approach by maximizing the so called \textbf{parameter likelihood}. In mathematical terms, let $p(x^{(1)}, \dotsc, x^{(n)} | \theta)$ be the joint density function for our IID data samples $x^{(1)}, \dotsc, x^{(n)} $ and some parameters $\theta$. Then this translates to finding $\arg\max_{\theta} p(x^{(1)}, \dotsc, x^{(n)}  | \theta) = \prod^n_i p(x^{(i)}| \theta)$. Note how we are considering our function fixed on the data points as we are trying to find the "best fit" for them.
	
	Computationally, to avoid numerical instability and to be able to conduct optimiziation, we typically aim to minimize quantities. We then can transform our problem: we can take $\text{NLL}(\theta) = -\log p(x^1, \dotsc, x^n | \theta)$. Finding the maximum parameter likelihood is then equivallent (due to logarithmic injectivity) to finding $\arg\min_{\theta} \text{NLL}(\theta)$.
	
	\textbf{Observation:} This statistical concept is closely tied to typical the optimization process in DL, for which we use a loss function to update the weights of our models. The NLL is a bridge between statistics and DL.
	
	

	\subsection{Loss functions}
	
	\subsubsection{Cross-Entropy Loss}
	
	Before we get into detail, let us borrow a very important concept from statistics that help us understand better what cross-entropy is:
	
	\noindent
	\vspace{0.5px}
	\textbf{Kullback-Leibler Divergence}

	\vspace{0.5px}
	The \textbf{Kullback-Leibler Divergence} $D_{KL}(P || Q)$ is a measure of the information lost when $Q$ is used to approximate $P$. In more of a mathematical approach we say it is the "distance" between both probability distributions. The formula for discrete distributions is:
	$$D_{KL}(P || Q) = \sum_{i \in \mathcal{X}} P(i) \log \left(\frac{P(i)}{Q(i)}\right)$$
	
	Let us motivate it through a concrete example. Given the two discrete probability distributions on the sample space $\mathcal{X} = \{0, 1\}$:
	\begin{itemize}
		\item \textbf{True Distribution $P$ (Fair Coin $X$):}
		$P(X=0) = P(0) = p_1$
		$P(X=1) = P(1) = q_1$
		\item \textbf{Approximating Distribution $Q$ (Biased Coin $Y$):}
		$P(Y=0) = Q(0) = p_2$
		$P(Y=1) = Q(1) = q_2$ 
	\end{itemize}
	
	We want to compare for a same event the probability of it happening. This in general cannot be done as they need not share the same sample space, but none the less it is a good way to visualize it.
	
	Consider that we flip the coin $N$ times. Then our sample space is completely determined by strings of this same length of the type $101 \dotsc 1$. For any given event $e \in \Omega$, we can define the ratio:
	
	$$
	\frac{P(e)}{Q(e)} = \frac{p_1^nq_1^m}{p_2^nq_2^m}
	$$
	
	
	Now we can take the $\log$ and divide by $N$:
	\begin{align*}
		\frac{1}{N} \log \left( \frac{P(e)}{Q(e)} \right) &= \frac{1}{N} \log \left( \frac{p_1^n q_1^m}{p_2^n q_2^m} \right) \\
		&= \frac{1}{N} \left[ \log(p_1^n) + \log(q_1^m) - \log(p_2^n) - \log(q_2^m) \right] \\
		&= \frac{1}{N} \left[ n \log(p_1) + m \log(q_1) - n \log(p_2) - m \log(q_2) \right] \\
		&= \frac{n}{N} \left[ \log(p_1) - \log(p_2) \right] + \frac{m}{N} \left[ \log(q_1) - \log(q_2) \right] \\
		&= \frac{n}{N} \log \left( \frac{p_1}{p_2} \right) + \frac{m}{N} \log \left( \frac{q_1}{q_2} \right)
	\end{align*}
	
	In the limit as $N \to \infty$, by the Law of Large Numbers, the frequency $\frac{n}{N}$ converges in probability to the true probability $P(1) = p_1$, and $\frac{m}{N}$ converges to $P(0) = q_1$.
	
	Thus, taking the expectation over the sequence of $N$ events, $\mathbb{E}_{P} \left[ \frac{1}{N} \log \left( \frac{P(e)}{Q(e)} \right) \right]$, we find that the limit of this average log-ratio is the Kullback-Leibler Divergence:
	
	$$
	\lim_{N \to \infty} \mathbb{E}_{P} \left[ \frac{1}{N} \log \left( \frac{P(e)}{Q(e)} \right) \right] = p_1 \log \left( \frac{p_1}{p_2} \right) + q_1 \log \left( \frac{q_1}{q_2} \right)
	$$
	
	This is the general formula for the KL-Divergence between two Bernoulli distributions:
	$$
	D_{KL}(P || Q) = \sum_{i \in \{0, 1\}} P(i) \log \left( \frac{P(i)}{Q(i)} \right)
	$$
	where $P(1)=p_1, P(0)=q_1$, $Q(1)=p_2, Q(0)=q_2$.	
	
	\subsection{Regularization}
	\subsection{Optimization methods}
	
	\newpage
	\section{Extra material}\label{subsec:extra_mat}
	
	\subsection{BPE}\label{extra:bpe}
	Here I include pseudocode for some of the functions in Algorithm~\ref{alg:bpe}, together with a python implementation of it.
	
	
		\begin{algorithm}[h!]
		\caption{Further functions for BPE Training}
		\label{alg:bpe-extra}
		\begin{algorithmic}[1]
			\Procedure{CountPairs}{$S$}
			\State $Counts \gets \text{empty map}$
			\For{$i \gets 0$ \textbf{to} $|S| - 2$}
			\State $pair \gets (S[i], S[i+1])$
			\State $Counts[pair] \gets Counts[pair] + 1$
			\EndFor
			\State \textbf{return} $Counts$
			\EndProcedure
			
			\Procedure{Merge}{$S, pair, new\_token$}
			\State $S_{new} \gets []$
			\State $i \gets 0$
			\While{$i < |S|$}
			\If{$i < |S| - 1 \textbf{ and } S[i] == pair[0] \textbf{ and } S[i+1] == pair[1]$}
			\State Append $new\_token$ to $S_{new}$
			\State $i \gets i + 2$
			\Else
			\State Append $S[i]$ to $S_{new}$
			\State $i \gets i + 1$
			\EndIf
			\EndWhile
			\State \textbf{return} $S_{new}$
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	

	
	\gitcode{tokenizer.py}{bpetraining}{An implementation of BPE in Python (adapted code from \href{https://github.com/karpathy/minbpe}{Andrej Karpathy's github} )}{lst:bpe}
	
	
	\section{Annex}

	\subsection{Pseudo-likelihood}
	
	
	
\end{document}